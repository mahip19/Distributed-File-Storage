\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{booktabs} % For nicer tables
\usepackage{dirtree}  % For file structure tree
\usepackage[hidelinks]{hyperref}
\usepackage{titling}
\usepackage{verbatim}
\usepackage{amsmath}

% Formatting setup
\setlist[itemize]{leftmargin=1.2cm, label={--}, itemsep=4pt}
\setlist[enumerate]{leftmargin=1.2cm, itemsep=4pt}
\renewcommand{\baselinestretch}{1.1}

\title{\textbf{Project Status Report: Distributed File Storage System}}
\author{Samyak Shah \and Mahip Parekh}
\date{CS 6650: Building Scalable Distributed Systems \\ November 2025}

\begin{document}

\maketitle
\thispagestyle{empty}
\vspace{-1em}
\hrule
\vspace{1em}

\section*{1. Executive Summary}
This project implements a distributed file storage system that splits files into chunks and distributes them across multiple storage nodes. The system utilizes \textbf{Consistent Hashing (DHT)} for efficient chunk distribution and \textbf{Chain Replication} to ensure strong consistency for file metadata. This architecture was chosen to demonstrate core distributed systems concepts including fault tolerance, data replication, and decentralized coordination, pivoting from an initial blockchain-based design to focus on linearizable consistency and availability.

\section{2. Design Evolution and Scope Adjustment}
\subsection*{2.1 Rationale for Pivot}
The original proposal aimed to use a lightweight blockchain for metadata management. Following instructor feedback regarding scope and timeline, we simplified the design to use a benign consensus model.

\subsection*{2.2 Selected Approach: Chain Replication}
We selected Chain Replication (referencing van Renesse \& Schneider) over other consensus mechanisms (like Raft or Paxos) for the following reasons:
\begin{itemize}
    \item \textbf{Strong Consistency:} Guarantees linearizability; all reads from the tail see the latest write.
    \item \textbf{Simplicity:} Easier to implement and reason about failure recovery compared to Multi-Paxos.
    \item \textbf{Suitability:} Perfectly fits our 3-node metadata cluster setup ($Head \rightarrow Mid \rightarrow Tail$).
\end{itemize}

\section{3. System Architecture}
The system is divided into three distinct layers.

\subsection*{3.1 Component Responsibilities}
\begin{table}[h]
    \centering
    \begin{tabular}{@{}ll@{}}
    \toprule
    \textbf{Component} & \textbf{Responsibility} \\ \midrule
    \textbf{Client} & File chunking (1MB), CID computation (SHA-256), Orchestration. \\
    \textbf{Metadata Layer} & Stores file-to-chunk mappings with strong consistency via Chain Repl. \\
    \textbf{Storage Layer} & Stores actual chunk data; handles replication. \\
    \textbf{DHT} & Determines which node stores which chunk (Consistent Hashing). \\ \bottomrule
    \end{tabular}
    \caption{System Components and Responsibilities}
\end{table}

\subsection*{3.2 Data Flow and Logic}
\begin{itemize}
    \item \textbf{Client:} Splits files into 1MB pieces and computes a SHA-256 Root Hash (CID). Identical content results in an identical CID (Content-Addressable Storage).
    \item \textbf{Metadata (Chain Replication):} 
    $$ \text{HEAD} \rightarrow \text{MID} \rightarrow \text{TAIL} $$
    Writes originate at the HEAD and propagate to the TAIL. Reads are served exclusively by the TAIL to guarantee consistency.
    \item \textbf{Storage (DHT Ring):} Chunks are mapped to nodes using a consistent hash ring.
\end{itemize}

\section{4. Technical Decisions}

\subsection*{4.1 Consistent Hashing vs. Simple Modulo}
We chose consistent hashing to minimize data movement during scaling events.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{Aspect} & \textbf{Hash \% N (Modulo)} & \textbf{Consistent Hashing} \\ \midrule
    \textbf{Node Addition} & $\sim$90\% data moves (for $N=10$) & $\sim$1/N data moves \\
    \textbf{Implementation} & Simple & Moderate (Ring + Virtual Nodes) \\
    \textbf{Scalability} & Poor & Excellent \\ \bottomrule
    \end{tabular}
    \caption{Comparison of Partitioning Strategies}
\end{table}

\subsection*{4.2 Storage Strategy}
We are using \textbf{In-Memory Storage} (using Java `ConcurrentHashMap`) rather than writing to the disk of the Khoury servers.
\begin{itemize}
    \item \textbf{Rationale:} This strictly simulates a distributed environment where process death equals data loss. If we used the shared NFS on the cluster, we would not effectively demonstrate the need for network-based replication.
\end{itemize}

\section{5. Implementation Status}

\subsection*{5.1 Completed Work}
\begin{itemize}
    \item \textbf{Chunking System:} Files are split into 1MB chunks using `java.io` streams; handles arbitrary file sizes.
    \item \textbf{Hashing:} SHA-256 implementation to generate Content IDs (CIDs).
    \item \textbf{Network Layer:} TCP Client-Server foundation using `java.net.Socket` with `DataOutputStream` for length-prefixed messaging.
    \item \textbf{DHT Module:} Ring-based consistent hashing implemented with $O(\log n)$ lookups using `java.util.TreeMap` (specifically the `ceilingEntry` method).
    \item \textbf{Verification Utilities:} Tools to verify file reconstruction matches the original byte-for-byte.
\end{itemize}

\subsection*{5.2 Planned Work (Next Steps)}
\begin{itemize}
    \item \textbf{Storage Node Server:} Complete the RPC handlers for `STORE\_CHUNK`, `GET\_CHUNK`, and `CHUNK\_NOT\_FOUND` using thread pools.
    \item \textbf{Chunk Replication:} Implement logic to walk the hash ring clockwise to store replicas on $k$ nodes.
    \item \textbf{Chain Replication:} Implement the forwarding logic ($Head \rightarrow Mid \rightarrow Tail$) and failure handling (e.g., if Head fails, Mid becomes Head).
    \item \textbf{Cluster Deployment:} Deploy JAR executables to Khoury Linux servers (linux-079, linux-080, linux-081).
\end{itemize}

\section{6. Project Timeline}

\begin{table}[h]
    \centering
    \begin{tabular}{@{}llc@{}}
    \toprule
    \textbf{Week} & \textbf{Tasks} & \textbf{Status} \\ \midrule
    Week 1 & File chunking, hashing, TCP setup & \textbf{Complete} \\
    Week 2 & Consistent Hashing (DHT), Verification utils & \textbf{Complete} \\
    Week 3 & Storage node server, Chunk replication & \textit{In Progress} \\
    Week 4 & Chain replication for Metadata & \textit{Planned} \\
    Week 5 & Client integration, End-to-end testing & \textit{Planned} \\
    Week 6 & Cluster deployment, Demo, Final Report & \textit{Planned} \\ \bottomrule
    \end{tabular}
\end{table}


\vspace{2em}
\hrule

\end{document}